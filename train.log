loading training data...
txt/bb.train.txt
dataset size 28000
symbol vocab size: 3552
symbol vocab size(actual): 3549

epoch: 0 / 100
train mean loss=429.10642855
training perplexity=45599.8959518
saving model....
epoch: 1 / 100
train mean loss=323.239160876
training perplexity=3232.39627476
saving model....
epoch: 2 / 100
train mean loss=313.23347883
training perplexity=2517.03517684
saving model....
epoch: 3 / 100
train mean loss=308.679802529
training perplexity=2246.19974103
saving model....
epoch: 4 / 100
train mean loss=305.969572601
training perplexity=2099.04826751
saving model....
epoch: 5 / 100
train mean loss=304.083235059
training perplexity=2002.3582231
saving model....
epoch: 6 / 100
train mean loss=302.744230826
training perplexity=1936.43855912
saving model....
epoch: 7 / 100
train mean loss=301.744245714
training perplexity=1888.62842371
saving model....
epoch: 8 / 100
train mean loss=300.984119873
training perplexity=1853.07740243
saving model....
epoch: 9 / 100
train mean loss=300.264989515
training perplexity=1820.05998419
saving model....
epoch: 10 / 100
train mean loss=299.711222621
training perplexity=1795.03637535
saving model....
epoch: 11 / 100
train mean loss=299.316321389
training perplexity=1777.40201456
saving model....
epoch: 12 / 100
train mean loss=298.860060316
training perplexity=1757.24322026
saving model....
epoch: 13 / 100
train mean loss=298.547017038
training perplexity=1743.54456412
saving model....
epoch: 14 / 100
train mean loss=298.208360944
training perplexity=1728.845327
saving model....
epoch: 15 / 100
train mean loss=297.950621512
training perplexity=1717.74134925
saving model....
epoch: 16 / 100
train mean loss=297.711777693
training perplexity=1707.51511292
saving model....
epoch: 17 / 100
train mean loss=297.53336404
training perplexity=1699.91597269
saving model....
epoch: 18 / 100
train mean loss=297.263556649
training perplexity=1688.48830956
saving model....
epoch: 19 / 100
train mean loss=297.12803203
training perplexity=1682.7771966
saving model....
epoch: 20 / 100
train mean loss=296.897910352
training perplexity=1673.12390333
saving model....
epoch: 21 / 100
train mean loss=296.829954245
training perplexity=1670.28384183
saving model....
epoch: 22 / 100
train mean loss=296.60211938
training perplexity=1660.79716259
saving model....
epoch: 23 / 100
train mean loss=296.465419835
training perplexity=1655.13109452
saving model....
epoch: 24 / 100
train mean loss=296.335498352
training perplexity=1649.76388854
saving model....
epoch: 25 / 100
train mean loss=296.21251212
training perplexity=1644.69922247
saving model....
epoch: 26 / 100
train mean loss=296.101134644
training perplexity=1640.12603109
saving model....
epoch: 27 / 100
train mean loss=295.970091378
training perplexity=1634.76163622
saving model....
epoch: 28 / 100
train mean loss=295.878522993
training perplexity=1631.02360433
saving model....
epoch: 29 / 100
train mean loss=295.771036442
training perplexity=1626.64666022
saving model....
epoch: 30 / 100
train mean loss=295.756753278
training perplexity=1626.06592239
saving model....
epoch: 31 / 100
train mean loss=295.596234065
training perplexity=1619.55362743
saving model....
epoch: 32 / 100
train mean loss=295.519663304
training perplexity=1616.45633154
saving model....
epoch: 33 / 100
train mean loss=295.415004534
training perplexity=1612.2324515
saving model....
epoch: 34 / 100
train mean loss=295.313680158
training perplexity=1608.15365853
saving model....
epoch: 35 / 100
train mean loss=295.295830994
training perplexity=1607.43621362
saving model....
epoch: 36 / 100
train mean loss=295.137675389
training perplexity=1601.09313564
saving model....
epoch: 37 / 100
train mean loss=295.047842756
training perplexity=1597.50141003
saving model....
epoch: 38 / 100
train mean loss=295.028381609
training perplexity=1596.72436884
saving model....
epoch: 39 / 100
train mean loss=295.048163561
training perplexity=1597.51422225
saving model....
epoch: 40 / 100
train mean loss=294.864775957
training perplexity=1590.20687837
saving model....
epoch: 41 / 100
train mean loss=294.91306257
training perplexity=1592.12768008
saving model....
epoch: 42 / 100
train mean loss=294.826235177
training perplexity=1588.67542094
saving model....
epoch: 43 / 100
train mean loss=294.739254783
training perplexity=1585.22458386
saving model....
epoch: 44 / 100
train mean loss=294.752817579
training perplexity=1585.76217696
saving model....
epoch: 45 / 100
train mean loss=294.653949825
training perplexity=1581.84749827
saving model....
epoch: 46 / 100
train mean loss=294.637524523
training perplexity=1581.19807355
saving model....
epoch: 47 / 100
train mean loss=294.586969713
training perplexity=1579.20090671
saving model....
epoch: 48 / 100
train mean loss=294.465519257
training perplexity=1574.41331179
saving model....
epoch: 49 / 100
train mean loss=294.501139199
training perplexity=1575.81594903
saving model....
epoch: 50 / 100
train mean loss=294.414616961
training perplexity=1572.41105475
saving model....
epoch: 51 / 100
train mean loss=294.370526537
training perplexity=1570.67880289
saving model....
epoch: 52 / 100
train mean loss=294.357096579
training perplexity=1570.15153766
saving model....
epoch: 53 / 100
train mean loss=294.268226951
training perplexity=1566.66694041
saving model....
epoch: 54 / 100
train mean loss=294.248073861
training perplexity=1565.87780976
saving model....
epoch: 55 / 100
train mean loss=294.207137473
training perplexity=1564.27609495
saving model....
epoch: 56 / 100
train mean loss=294.161523176
training perplexity=1562.49327781
saving model....
epoch: 57 / 100
train mean loss=294.138978707
training perplexity=1561.61288642
saving model....
epoch: 58 / 100
train mean loss=294.157399336
training perplexity=1562.33219929
saving model....
epoch: 59 / 100
train mean loss=294.106556636
training perplexity=1560.34763116
saving model....
epoch: 60 / 100
train mean loss=294.123977901
training perplexity=1561.0273599
saving model....
epoch: 61 / 100
train mean loss=294.026648996
training perplexity=1557.23365014
saving model....
epoch: 62 / 100
train mean loss=294.044149802
training perplexity=1557.91512032
saving model....
epoch: 63 / 100
train mean loss=293.964339948
training perplexity=1554.80979485
saving model....
epoch: 64 / 100
train mean loss=293.997359183
training perplexity=1556.09379048
saving model....
epoch: 65 / 100
train mean loss=293.845722547
training perplexity=1550.20593702
saving model....
epoch: 66 / 100
train mean loss=293.918637194
training perplexity=1553.03433208
saving model....
epoch: 67 / 100
train mean loss=293.868581587
training perplexity=1551.09209567
saving model....
epoch: 68 / 100
train mean loss=293.826453836
training perplexity=1549.45935508
saving model....
epoch: 69 / 100
train mean loss=293.812951747
training perplexity=1548.93641992
saving model....
epoch: 70 / 100
train mean loss=293.81004218
training perplexity=1548.82375563
saving model....
epoch: 71 / 100
train mean loss=293.767083348
training perplexity=1547.16125705
saving model....
epoch: 72 / 100
train mean loss=293.734553005
training perplexity=1545.90352638
saving model....
epoch: 73 / 100
train mean loss=293.727087141
training perplexity=1545.61501567
saving model....
epoch: 74 / 100
train mean loss=293.760457938
training perplexity=1546.90501383
saving model....
epoch: 75 / 100
train mean loss=293.75959998
training perplexity=1546.8718347
saving model....
epoch: 76 / 100
train mean loss=293.677613068
training perplexity=1543.70450067
saving model....
epoch: 77 / 100
train mean loss=293.63153013
training perplexity=1541.92706376
saving model....
epoch: 78 / 100
train mean loss=293.602359641
training perplexity=1540.80300453
saving model....
epoch: 79 / 100
train mean loss=293.614894605
training perplexity=1541.28592797
saving model....
epoch: 80 / 100
train mean loss=293.56764328
training perplexity=1539.46630785
saving model....
epoch: 81 / 100
train mean loss=293.651287842
training perplexity=1542.68887567
saving model....
epoch: 82 / 100
train mean loss=293.557082585
training perplexity=1539.05991564
saving model....
epoch: 83 / 100
train mean loss=293.516755567
training perplexity=1537.50905513
saving model....
epoch: 84 / 100
train mean loss=293.526826477
training perplexity=1537.89620674
saving model....
epoch: 85 / 100
train mean loss=293.48111775
training perplexity=1536.13982851
saving model....
epoch: 86 / 100
train mean loss=293.468041295
training perplexity=1535.63772899
saving model....
epoch: 87 / 100
train mean loss=293.486239537
training perplexity=1536.33653562
saving model....
epoch: 88 / 100
train mean loss=293.474678802
training perplexity=1535.89257032
saving model....
epoch: 89 / 100
train mean loss=293.460107989
training perplexity=1535.33319209
saving model....
epoch: 90 / 100
train mean loss=293.467985622
training perplexity=1535.63559166
saving model....
epoch: 91 / 100
train mean loss=293.38638201
training perplexity=1532.50594986
saving model....
epoch: 92 / 100
train mean loss=293.364684601
training perplexity=1531.67489005
saving model....
epoch: 93 / 100
train mean loss=293.365736346
training perplexity=1531.71516386
saving model....
epoch: 94 / 100
train mean loss=293.361100398
training perplexity=1531.53765038
saving model....
epoch: 95 / 100
train mean loss=293.369409158
training perplexity=1531.85581287
saving model....
epoch: 96 / 100
train mean loss=293.316500615
training perplexity=1529.83094584
saving model....
epoch: 97 / 100
train mean loss=293.323696856
training perplexity=1530.10619641
saving model....
epoch: 98 / 100
train mean loss=293.261334773
training perplexity=1527.7225398
saving model....
epoch: 99 / 100
train mean loss=293.298848092
training perplexity=1529.15596041
saving model....
