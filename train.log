loading training data...
txt/bb.train.txt
dataset size 28000
symbol vocab size: 3552
symbol vocab size(actual): 3549

epoch: 0 / 10
train mean loss=428.005261579
training perplexity=44361.6900602
saving model....
epoch: 1 / 10
train mean loss=323.266492179
training perplexity=3234.60566956
saving model....
epoch: 2 / 10
train mean loss=312.8857804
training perplexity=2495.25076462
saving model....
epoch: 3 / 10
train mean loss=308.519072745
training perplexity=2237.19207067
saving model....
epoch: 4 / 10
train mean loss=305.844395163
training perplexity=2092.48969806
saving model....
epoch: 5 / 10
train mean loss=304.036027222
training perplexity=1999.996442
saving model....
epoch: 6 / 10
train mean loss=302.705345306
training perplexity=1934.55698835
saving model....
epoch: 7 / 10
train mean loss=301.69510304
training perplexity=1886.30954217
saving model....
epoch: 8 / 10
train mean loss=300.910078517
training perplexity=1849.65046602
saving model....
epoch: 9 / 10
train mean loss=300.2292453
training perplexity=1818.43429529
saving model....
