loading training data...
txt/bb.train.txt
dataset size 28000
symbol vocab size: 3552
symbol vocab size(actual): 3549

epoch: 0 / 10
train mean loss=1058.41616595
training perplexity=3.10175514076e+11
saving model....
epoch: 1 / 10
train mean loss=694.432753863
training perplexity=34650366.0087
saving model....
epoch: 2 / 10
train mean loss=571.780643964
training perplexity=1614468.08154
saving model....
epoch: 3 / 10
train mean loss=533.090201721
training perplexity=613696.044503
saving model....
epoch: 4 / 10
train mean loss=511.551347918
training perplexity=358177.408801
saving model....
epoch: 5 / 10
train mean loss=495.790338832
training perplexity=241532.295233
saving model....
epoch: 6 / 10
train mean loss=482.779406956
training perplexity=174466.00256
saving model....
epoch: 7 / 10
train mean loss=471.887272731
training perplexity=132877.352259
saving model....
epoch: 8 / 10
train mean loss=462.314132472
training perplexity=104595.238842
saving model....
epoch: 9 / 10
train mean loss=454.124400155
training perplexity=85230.1057163
saving model....
